{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore', category=ImportWarning)\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import edward2 as ed\n",
    "from tensorflow.estimator.inputs import numpy_input_fn\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_SECONDS = 15\n",
    "WAV_SAMPLE_RATE = 22050\n",
    "WAV_SHAPE = [WAV_SECONDS * WAV_SAMPLE_RATE, 1]  # Time-steps X Features\n",
    "\n",
    "LEARNING_RATE = 0.0005\n",
    "BATCH_SIZE = 1\n",
    "BASE_DEPTH = 12\n",
    "LATENT_DIMENSIONS = 8\n",
    "ACTIVATION = \"leaky_relu\"\n",
    "\n",
    "\n",
    "MODEL_DIR=\"/data/tensorflow/individual_vae\"\n",
    "DATA_DIR=\"{}/data\".format(MODEL_DIR)\n",
    "MAX_STEPS=40\n",
    "VIZ_STEPS=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _softplus_inverse(x):\n",
    "  \"\"\"Helper which computes the function inverse of `tf.nn.softplus`.\"\"\"\n",
    "  return tf.log(tf.math.expm1(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoders aka Inference Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cnn_encoder(activation, latent_size, base_depth):\n",
    "  \"\"\"Creates the encoder function.\n",
    "  Args:\n",
    "    activation: Activation function in hidden layers.\n",
    "    latent_size: The dimensionality of the encoding.\n",
    "    base_depth: The lowest depth for a layer.\n",
    "  Returns:\n",
    "    encoder: A `callable` mapping a `Tensor` of images to a\n",
    "      `tfd.Distribution` instance over encodings.\n",
    "  \"\"\"\n",
    "  conv = functools.partial(\n",
    "      tf.keras.layers.Conv1D, padding=\"CAUSAL\", activation=activation)\n",
    "\n",
    "  encoder_net = tf.keras.Sequential([\n",
    "      conv(base_depth, 5, 1),\n",
    "      conv(base_depth, 5, 2),\n",
    "      conv(2 * base_depth, 5, 1),\n",
    "      conv(2 * base_depth, 5, 2),\n",
    "      conv(4 * latent_size, 3, padding=\"VALID\"),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(2*latent_size, activation=None),\n",
    "  ])\n",
    "\n",
    "  def encoder(images):\n",
    "    images = tf.reshape(images, (-1, WAV_SHAPE[0], 1))\n",
    "    net = encoder_net(images)\n",
    "    print(\"NET SHAPE: {}\".format(net.shape))\n",
    "\n",
    "    return tfd.MultivariateNormalDiag(\n",
    "        loc=net[..., :latent_size],\n",
    "        scale_diag=tf.nn.softplus(net[..., latent_size:] +\n",
    "                                  _softplus_inverse(1.0)),\n",
    "        name=\"code\")\n",
    "    \"\"\"\n",
    "    return tfd.Normal(\n",
    "        loc=net[..., :latent_size],\n",
    "        scale=tf.nn.softplus(net[..., latent_size:] + _softplus_inverse(1.0)),\n",
    "        name=\"code\")\n",
    "    \"\"\"\n",
    "\n",
    "  return encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoders aka Generative Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cnn_decoder(activation, latent_size, output_shape, base_depth):\n",
    "  \"\"\"Creates the decoder function.\n",
    "  Args:\n",
    "    activation: Activation function in hidden layers.\n",
    "    latent_size: Dimensionality of the encoding.\n",
    "    output_shape: The output image shape.\n",
    "    base_depth: Smallest depth for a layer.\n",
    "  Returns:\n",
    "    decoder: A `callable` mapping a `Tensor` of encodings to a\n",
    "      `tfd.Distribution` instance over images.\n",
    "  \"\"\"\n",
    "  conv = functools.partial(\n",
    "      tf.keras.layers.Conv1D, padding=\"CAUSAL\", activation=activation)\n",
    "    \n",
    "  decoder_net = tf.keras.Sequential([\n",
    "      conv(2 * base_depth, 7, padding=\"VALID\"),\n",
    "      tf.keras.layers.UpSampling1D(size=2),\n",
    "      conv(2 * base_depth, 5),\n",
    "      tf.keras.layers.UpSampling1D(size=2),\n",
    "      conv(2 * base_depth, 5, 2),\n",
    "      tf.keras.layers.UpSampling1D(size=2),\n",
    "      conv(base_depth, 5),\n",
    "      tf.keras.layers.UpSampling1D(size=2),\n",
    "      conv(base_depth, 5, 2),\n",
    "      tf.keras.layers.UpSampling1D(size=2),\n",
    "      conv(base_depth, 5),\n",
    "      tf.keras.layers.UpSampling1D(size=2),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(2*WAV_SHAPE[0], activation=None),\n",
    "  ])\n",
    "\n",
    "  def decoder(codes):\n",
    "    original_shape = tf.shape(codes)\n",
    "    codes = tf.reshape(codes, (-1, latent_size, 1))\n",
    "    net = decoder_net(codes)\n",
    "    print(\"DECODER NET SHAPE: {}\".format(net.shape))\n",
    "    \"\"\"\n",
    "    return tfd.Independent(tfd.Bernoulli(logits=logits),\n",
    "                           reinterpreted_batch_ndims=len(output_shape),\n",
    "                           name=\"image\")\n",
    "    \"\"\"\n",
    "    \n",
    "    return tfd.Normal(\n",
    "        loc=net[..., :WAV_SHAPE[0]],\n",
    "        scale=tf.nn.softplus(net[..., WAV_SHAPE[0]:] + _softplus_inverse(1.0)),\n",
    "        name=\"wav\")\n",
    "\n",
    "    \"\"\"\n",
    "    return tfd.MultivariateNormalDiag(\n",
    "        loc=net[..., :WAV_SHAPE[0]],\n",
    "        scale_diag=tf.nn.softplus(net[..., WAV_SHAPE[0]:] +\n",
    "                                  _softplus_inverse(1.0)),\n",
    "        name=\"wav\")\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "  return decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Estimator model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode, params, config):\n",
    "  \"\"\"Builds the model function for use in an estimator.\n",
    "  Arguments:\n",
    "    features: The input features for the estimator.\n",
    "    labels: The labels, unused here.\n",
    "    mode: Signifies whether it is train or test or predict.\n",
    "    params: Some hyperparameters as a dictionary.\n",
    "    config: The RunConfig, unused here.\n",
    "  Returns:\n",
    "    EstimatorSpec: A tf.estimator.EstimatorSpec instance.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  encoder = make_cnn_encoder(params[\"activation\"],\n",
    "                             params[\"latent_size\"],\n",
    "                             params[\"base_depth\"])\n",
    "  decoder = make_cnn_decoder(params[\"activation\"],\n",
    "                             params[\"latent_size\"],\n",
    "                             WAV_SHAPE,\n",
    "                             params[\"base_depth\"])\n",
    "  latent_prior = tfd.MultivariateNormalDiag(\n",
    "        loc=tf.zeros([params[\"latent_size\"]]),\n",
    "        scale_identity_multiplier=1.0\n",
    "  )\n",
    "\n",
    "  approx_posterior = encoder(features)\n",
    "  approx_posterior_sample = approx_posterior.sample(1)#params[\"n_samples\"])\n",
    "  decoder_likelihood = decoder(approx_posterior_sample)\n",
    "\n",
    "  # `distortion` is just the negative log likelihood.\n",
    "  distortion = -decoder_likelihood.log_prob(features)\n",
    "  avg_distortion = tf.reduce_mean(distortion)\n",
    "  tf.summary.scalar(\"distortion\", avg_distortion)\n",
    "\n",
    "  approx_posterior_log_prob = approx_posterior.log_prob(approx_posterior_sample)\n",
    "  latent_prior_log_prob = latent_prior.log_prob(approx_posterior_sample)\n",
    "  rate = (approx_posterior_log_prob -  latent_prior_log_prob)\n",
    "  avg_rate = tf.reduce_mean(rate)\n",
    "  tf.summary.scalar(\"rate\", avg_rate)\n",
    "\n",
    "  elbo_local = -(rate + distortion)\n",
    "  elbo = tf.reduce_mean(elbo_local)\n",
    "  #elbo = -(avg_rate + avg_distortion)\n",
    "  loss = -elbo\n",
    "  tf.summary.scalar(\"elbo\", elbo)\n",
    "\n",
    "\n",
    "  importance_weighted_elbo = tf.reduce_mean(\n",
    "      tf.reduce_logsumexp(elbo_local, axis=0) -\n",
    "      tf.log(tf.to_float(1)))\n",
    "  tf.summary.scalar(\"elbo/importance_weighted\", importance_weighted_elbo)\n",
    "\n",
    "    \n",
    "  random_wav = decoder(latent_prior.sample(16))\n",
    "  #random_wav = decoder(approx_posterior.sample(16))\n",
    "  tf.summary.audio(\"random/sample\", random_wav.sample(), sample_rate=22050)\n",
    "  tf.summary.audio(\"random/mean\", random_wav.mean(), sample_rate=22050)\n",
    "\n",
    "  # Perform variational inference by minimizing the -ELBO.\n",
    "  global_step = tf.train.get_or_create_global_step()\n",
    "  learning_rate = tf.train.cosine_decay(params[\"learning_rate\"], global_step,\n",
    "                                        params[\"max_steps\"])\n",
    "  tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "  optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "  train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    predictions = {\n",
    "        'encoded_sample': approx_posterior.sample(1), \n",
    "        'encoded_mean': approx_posterior.mean(), \n",
    "        'reconstructed_sample': decoder_likelihood.sample(1), \n",
    "        'reconstructed_mean': decoder_likelihood.mean(),\n",
    "        'log_likelihood': -avg_distortion,\n",
    "        'random_wav': random_wav.mean(),\n",
    "    }\n",
    "  else:\n",
    "    predictions = None\n",
    "\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      loss=loss,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={\n",
    "          \"elbo\": tf.metrics.mean(elbo),\n",
    "          \"elbo/importance_weighted\": tf.metrics.mean(importance_weighted_elbo),\n",
    "          \"rate\": tf.metrics.mean(avg_rate),\n",
    "          \"distortion\": tf.metrics.mean(avg_distortion),\n",
    "      },\n",
    "      predictions=predictions,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(train_input_fn, eval_input_fn=None, model_dir=MODEL_DIR):\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'latent_size': LATENT_DIMENSIONS,\n",
    "        'activation': ACTIVATION,\n",
    "        'base_depth': BASE_DEPTH,\n",
    "        'max_steps': MAX_STEPS,\n",
    "    }\n",
    "    params[\"activation\"] = getattr(tf.nn, params[\"activation\"])\n",
    "    \"\"\"\n",
    "    if FLAGS.delete_existing and tf.gfile.Exists(MODEL_DIR):\n",
    "        tf.logging.warn(\"Deleting old log directory at {}\".format(MODEL_DIR))\n",
    "        tf.gfile.DeleteRecursively(MODEL_DIR)\n",
    "        tf.gfile.MakeDirs(MODEL_DIR)\n",
    "    \"\"\"\n",
    "    \n",
    "    estimator = tf.estimator.Estimator(\n",
    "      cnn_model_fn,\n",
    "      params=params,\n",
    "      config=tf.estimator.RunConfig(\n",
    "          model_dir=model_dir,\n",
    "          save_checkpoints_steps=VIZ_STEPS,\n",
    "      ),\n",
    "    )\n",
    "    for _ in range(MAX_STEPS // VIZ_STEPS):\n",
    "        estimator.train(input_fn=train_input_fn, steps=VIZ_STEPS)\n",
    "        if eval_input_fn:\n",
    "            estimator.evaluate(input_fn=eval_input_fn)\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data(file_name):\n",
    "    import feather  # Super fast way to read/write tabular data\n",
    "    training_data = feather.read_dataframe(file_name).set_index('index')\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dir: /data/tensorflow/individual_vae/Acoustic_guitar\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/data/tensorflow/individual_vae/Acoustic_guitar', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 20, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa30af4ea20>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From /home/smcclain1/venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /data/tensorflow/individual_vae/Acoustic_guitar/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.91323507, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 20 into /data/tensorflow/individual_vae/Acoustic_guitar/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.6885911.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-28-04:35:12\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Acoustic_guitar/model.ckpt-20\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-28-04:35:17\n",
      "INFO:tensorflow:Saving dict for global step 20: distortion = 0.89614457, elbo = -1.3031505, elbo/importance_weighted = -1.3031505, global_step = 20, loss = 1.3031505, rate = 0.4069849\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 20: /data/tensorflow/individual_vae/Acoustic_guitar/model.ckpt-20\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Acoustic_guitar/model.ckpt-20\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 20 into /data/tensorflow/individual_vae/Acoustic_guitar/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.28942302, step = 21\n",
      "INFO:tensorflow:Saving checkpoints for 40 into /data/tensorflow/individual_vae/Acoustic_guitar/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.4462975.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-28-04:36:03\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Acoustic_guitar/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-28-04:36:08\n",
      "INFO:tensorflow:Saving dict for global step 40: distortion = 0.8643072, elbo = -1.3741801, elbo/importance_weighted = -1.3741801, global_step = 40, loss = 1.3741801, rate = 0.5098647\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 40: /data/tensorflow/individual_vae/Acoustic_guitar/model.ckpt-40\n",
      "Model dir: /data/tensorflow/individual_vae/Clarinet\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/data/tensorflow/individual_vae/Clarinet', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 20, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa3242c52b0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /data/tensorflow/individual_vae/Clarinet/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.92161185, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 20 into /data/tensorflow/individual_vae/Clarinet/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.1057295.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-28-04:37:15\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Clarinet/model.ckpt-20\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-28-04:37:19\n",
      "INFO:tensorflow:Saving dict for global step 20: distortion = 0.8984869, elbo = -57.67284, elbo/importance_weighted = -57.67284, global_step = 20, loss = 57.67284, rate = 56.796272\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 20: /data/tensorflow/individual_vae/Clarinet/model.ckpt-20\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Clarinet/model.ckpt-20\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 20 into /data/tensorflow/individual_vae/Clarinet/model.ckpt.\n",
      "INFO:tensorflow:loss = 5.5512276, step = 21\n",
      "INFO:tensorflow:Saving checkpoints for 40 into /data/tensorflow/individual_vae/Clarinet/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 152.47264.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-28-04:38:06\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Clarinet/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-28-04:38:11\n",
      "INFO:tensorflow:Saving dict for global step 40: distortion = 0.87697685, elbo = -38.750435, elbo/importance_weighted = -38.750435, global_step = 40, loss = 38.750435, rate = 37.87832\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 40: /data/tensorflow/individual_vae/Clarinet/model.ckpt-40\n",
      "Model dir: /data/tensorflow/individual_vae/Flute\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/data/tensorflow/individual_vae/Flute', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 20, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa327c19198>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /data/tensorflow/individual_vae/Flute/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.9150141, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 20 into /data/tensorflow/individual_vae/Flute/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 11.559071.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-28-04:39:18\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Flute/model.ckpt-20\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-28-04:39:23\n",
      "INFO:tensorflow:Saving dict for global step 20: distortion = 0.8739497, elbo = -4.0047116, elbo/importance_weighted = -4.0047116, global_step = 20, loss = 4.0047116, rate = 3.1309059\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 20: /data/tensorflow/individual_vae/Flute/model.ckpt-20\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Flute/model.ckpt-20\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 20 into /data/tensorflow/individual_vae/Flute/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.59988606, step = 21\n",
      "INFO:tensorflow:Saving checkpoints for 40 into /data/tensorflow/individual_vae/Flute/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.4962363.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-28-04:40:10\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Flute/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-28-04:40:15\n",
      "INFO:tensorflow:Saving dict for global step 40: distortion = 0.8268727, elbo = -3.664714, elbo/importance_weighted = -3.664714, global_step = 40, loss = 3.664714, rate = 2.838078\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 40: /data/tensorflow/individual_vae/Flute/model.ckpt-40\n",
      "Model dir: /data/tensorflow/individual_vae/Applause\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/data/tensorflow/individual_vae/Applause', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 20, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa32b260748>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /data/tensorflow/individual_vae/Applause/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.942499, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 20 into /data/tensorflow/individual_vae/Applause/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.2925541.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-28-04:41:18\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Applause/model.ckpt-20\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-28-04:41:23\n",
      "INFO:tensorflow:Saving dict for global step 20: distortion = 0.91368705, elbo = -6.614193, elbo/importance_weighted = -6.614193, global_step = 20, loss = 6.614193, rate = 5.700519\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 20: /data/tensorflow/individual_vae/Applause/model.ckpt-20\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Applause/model.ckpt-20\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 20 into /data/tensorflow/individual_vae/Applause/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.5129821, step = 21\n",
      "INFO:tensorflow:Saving checkpoints for 40 into /data/tensorflow/individual_vae/Applause/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.1014818.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-28-04:42:09\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Applause/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-28-04:42:14\n",
      "INFO:tensorflow:Saving dict for global step 40: distortion = 0.9067184, elbo = -3.8338058, elbo/importance_weighted = -3.8338058, global_step = 40, loss = 3.8338058, rate = 2.927043\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 40: /data/tensorflow/individual_vae/Applause/model.ckpt-40\n",
      "Model dir: /data/tensorflow/individual_vae/Laughter\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/data/tensorflow/individual_vae/Laughter', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 20, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa329002588>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /data/tensorflow/individual_vae/Laughter/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.92261064, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 20 into /data/tensorflow/individual_vae/Laughter/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: -0.21356396.\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-28-04:43:20\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Laughter/model.ckpt-20\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-28-04:43:25\n",
      "INFO:tensorflow:Saving dict for global step 20: distortion = 0.907132, elbo = -1.2392744, elbo/importance_weighted = -1.2392744, global_step = 20, loss = 1.2392744, rate = 0.33210343\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 20: /data/tensorflow/individual_vae/Laughter/model.ckpt-20\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Laughter/model.ckpt-20\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 20 into /data/tensorflow/individual_vae/Laughter/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.5303291, step = 21\n",
      "INFO:tensorflow:Saving checkpoints for 40 into /data/tensorflow/individual_vae/Laughter/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.9062152.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-28-04:44:11\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Laughter/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-28-04:44:16\n",
      "INFO:tensorflow:Saving dict for global step 40: distortion = 0.8952956, elbo = -1.6083933, elbo/importance_weighted = -1.6083933, global_step = 40, loss = 1.6083933, rate = 0.7129531\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 40: /data/tensorflow/individual_vae/Laughter/model.ckpt-40\n"
     ]
    }
   ],
   "source": [
    "for label in ['Acoustic_guitar', 'Clarinet', 'Flute', 'Applause', 'Laughter']:\n",
    "#for label in ['Clarinet', 'Flute', 'Applause', 'Laughter']:\n",
    "    train_data = load_preprocessed_data('audio_train/padded_train_15s_{}_160.feather'.format(label))\n",
    "    y_train = train_data['label']\n",
    "    x_train = train_data.drop(['label', 'manually_verified'], axis=1)\n",
    "    test_data = load_preprocessed_data('audio_train/padded_test_15s_{}.feather'.format(label))\n",
    "    y_test = test_data['label']\n",
    "    x_test = test_data.drop(['label', 'manually_verified'], axis=1)\n",
    "    \n",
    "    train_input_fn = numpy_input_fn(\n",
    "        x_train.values.astype(np.float32), \n",
    "        shuffle=True, \n",
    "        batch_size=1\n",
    "    )\n",
    "    \n",
    "    eval_input_fn = numpy_input_fn(\n",
    "        x_test.values.astype(np.float32), \n",
    "        shuffle=True, \n",
    "        batch_size=1\n",
    "    )\n",
    "    model_dir = os.path.join(MODEL_DIR,label)\n",
    "    print(\"Model dir: {}\".format(model_dir))\n",
    "    \n",
    "    estimator = train_cnn(train_input_fn, eval_input_fn, model_dir)\n",
    "    estimators.append((label, estimator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag = feather.read_dataframe('padded_train_sample.feather').set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic = ag.loc[ag['label']=='Acoustic_guitar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = acoustic.drop(['label', 'manually_verified'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_preprocessed_data('audio_train/padded_train_15s_Acoustic_guitar_160.feather')\n",
    "y_train = train_data['label']\n",
    "x_train = train_data.drop(['label', 'manually_verified'], axis=1)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_preprocessed_data('audio_train/padded_test_15s_Acoustic_guitar.feather')\n",
    "y_test = test_data['label']\n",
    "x_test = test_data.drop(['label', 'manually_verified'], axis=1)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mport feather\n",
    "#f = feather.read_dataframe('librosa_train.feather').set_index('index')\n",
    "#f.loc[df['label'] == 'Acoustic_guitar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = numpy_input_fn(\n",
    "    x_train.values.astype(np.float32), \n",
    "    shuffle=True, \n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input_fn = numpy_input_fn(\n",
    "    x_test.values.astype(np.float32), \n",
    "    shuffle=True, \n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = train_cnn(train_input_fn, None)#eval_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Criticism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = np.array([x_test.iloc[i].astype(np.float32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_fn = numpy_input_fn(\n",
    "    ex, \n",
    "    shuffle=False, \n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Acoustic_guitar/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'encoded_sample': array([[[-0.6095326 , -0.652659  , -0.69579613,  0.29838806,\n",
       "            0.25552207,  0.09149103, -1.6592124 ,  0.29482937]]],\n",
       "        dtype=float32),\n",
       "  'encoded_mean': array([[-0.10565343, -0.07591414,  0.01060685,  0.03267853,  0.16968602,\n",
       "           0.14862823, -0.07775313,  0.05574958]], dtype=float32),\n",
       "  'reconstructed_sample': array([[[-0.37584585, -2.3290699 , -0.978498  , ..., -0.11236154,\n",
       "           -0.7556611 ,  0.48820338]]], dtype=float32),\n",
       "  'reconstructed_mean': array([[ 0.00229887, -0.00315949, -0.00090564, ...,  0.00077478,\n",
       "          -0.00399355, -0.00254196]], dtype=float32),\n",
       "  'log_likelihood': -0.86055076,\n",
       "  'random_wav': array([[ 2.0726852e-03, -3.2971250e-03, -1.3523323e-03, ...,\n",
       "           8.1298471e-04, -3.7084175e-03, -2.5208769e-03],\n",
       "         [ 2.3172922e-03, -3.6662705e-03, -1.1740256e-03, ...,\n",
       "           1.0616041e-03, -4.3817814e-03, -2.5132145e-03],\n",
       "         [ 2.0424835e-03, -3.9937277e-03, -1.8210706e-03, ...,\n",
       "           1.3002637e-03, -4.5497045e-03, -3.3698766e-03],\n",
       "         ...,\n",
       "         [ 2.2594456e-03, -2.7840678e-03, -5.7677098e-04, ...,\n",
       "           1.4332065e-05, -3.7205764e-03, -1.8755562e-03],\n",
       "         [ 2.8407543e-03, -3.3355104e-03, -6.9528434e-04, ...,\n",
       "           2.0238553e-04, -3.7657325e-03, -1.6521559e-03],\n",
       "         [ 2.8527433e-03, -4.3493728e-03, -6.1363087e-04, ...,\n",
       "           9.3186850e-04, -5.3281952e-03, -2.8198240e-03]], dtype=float32)}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(estimators[0][1].predict(predict_input_fn, yield_single_examples=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 330750)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 330750)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input_fn = numpy_input_fn(\n",
    "    ex, \n",
    "    shuffle=false, \n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_estimators(new_point, estimator_tuples):\n",
    "    max_likelihood = None\n",
    "    pred_label = None\n",
    "    ex = np.array([x_test.iloc[i].astype(np.float32)])\n",
    "    predict_input_fn = numpy_input_fn(\n",
    "        ex, \n",
    "        shuffle=False, \n",
    "        batch_size=1\n",
    "    )\n",
    "    for label, estimator in estimator_tuples:\n",
    "        pred = list(estimator.predict(predict_input_fn, yield_single_examples=False))[0]\n",
    "        likelihood = pred['log_likelihood']\n",
    "        if max_likelihood is None or likelihood > max_likelihood:\n",
    "            pred_label = label\n",
    "            max_likelihood = likelihood\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Acoustic_guitar/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Clarinet/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Flute/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Applause/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Laughter/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "CORRECT: 0, INCORRECT: 1, LABEL: Acoustic_guitar, PRED_LABEL: Flute\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Acoustic_guitar/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Clarinet/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Flute/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Applause/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Laughter/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "CORRECT: 0, INCORRECT: 2, LABEL: Acoustic_guitar, PRED_LABEL: Flute\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Acoustic_guitar/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Clarinet/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Flute/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/individual_vae/Applause/model.ckpt-40\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "NET SHAPE: (?, 16)\n",
      "DECODER NET SHAPE: (?, 661500)\n",
      "DECODER NET SHAPE: (16, 661500)\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "pred_labels = []\n",
    "for label in ['Acoustic_guitar', 'Clarinet', 'Flute', 'Applause', 'Laughter']:\n",
    "    test_data = load_preprocessed_data('audio_train/padded_test_15s_{}.feather'.format(label))\n",
    "    y_test = test_data['label']\n",
    "    x_test = test_data.drop(['label', 'manually_verified'], axis=1)\n",
    "    for i in range(x_test.shape[0]):\n",
    "        pred_label = predict_estimators(x_test.iloc[i].values, estimators)\n",
    "        pred_labels.append(pred_label)\n",
    "        if pred_label == y_test.iloc[i]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "        print(\"CORRECT: {}, INCORRECT: {}, LABEL: {}, PRED_LABEL: {}\".format(correct, incorrect, label, pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Flute']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_fn = numpy_input_fn(\n",
    "    x_train.values.astype(np.float32), \n",
    "    shuffle=False, \n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list(\n",
    "    estimator.predict(input_fn=predict_input_fn, yield_single_examples=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]['random_wav'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predictions)):\n",
    "    sample = predictions[i]['random_wav'][0]\n",
    "    librosa.output.write_wav('generated/{}.wav'.format(i), sample, WAV_SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = x_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_encodings = pd.DataFrame([predictions[i]['encoded_sample'][0][0] for i in range(len(predictions))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = y_train.to_frame()\n",
    "y_df['label_cat'] = y_df['label'].astype('category')\n",
    "y_df['label_int'] = y_df['label_cat'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(predicted_encodings.values, y_df['label_int'].values)\n",
    "lr.score(predicted_encodings.values, y_df['label_int'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the latent representation is not linearly separable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = RandomForestClassifier()\n",
    "r.fit(predicted_encodings.values, y_df['label_int'].values)\n",
    "r.score(predicted_encodings.values, y_df['label_int'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's more like it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO repeat the tests on *OUT OF SAMPLE* data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_padding(descriptive_df, mode='train'):\n",
    "    # '' using 15 second as our standard for padded audio files\n",
    "    # '' this function only returns a list of np.arrays without any label association (for faster run time)\n",
    "    names = []\n",
    "    audio = []\n",
    "    seconds = 15\n",
    "    sample_rate = 22050\n",
    "    max_len = seconds * sample_rate\n",
    "    for row in descriptive_df.itertuples():\n",
    "        file_path = 'audio_{}/audio_{}/{}'.format(mode, mode, row.Index)\n",
    "        data = librosa.load(file_path)[0][:max_len]\n",
    "        duration = data.shape[0]\n",
    "        padding_len = max_len - duration\n",
    "        padding = np.zeros(padding_len)\n",
    "        data = np.append(data, padding)\n",
    "        audio.append(data)\n",
    "        names.append(row.Index)\n",
    "    return pd.DataFrame(data=audio, index=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(label, count=None):\n",
    "    data = descriptive_df.loc[descriptive_df.label == label]\n",
    "    if count:\n",
    "        data = data.sample(n=count, replace=False)\n",
    "    return audio_padding(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive_df = pd.read_csv('train_descriptive.csv', index_col=0).set_index('fname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random 20 WAV files for each label\n",
    "train_df = pd.concat([get_df(l, 20) for l in descriptive_df['label'].unique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.join(descriptive_df[['label', 'manually_verified']])\n",
    "train_df = train_df.reset_index()\n",
    "train_df.columns = [str(col) for col in train_df.columns]\n",
    "train_df.to_feather('padded_train_15_sample_v2.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in descriptive_df['label'].unique():\n",
    "    print(\"Prepping training set for label: {}\".format(label))\n",
    "    train_df = get_df(label)\n",
    "    train_df = train_df.join(descriptive_df[['label', 'manually_verified']])\n",
    "    train_df = train_df.reset_index()\n",
    "    train_df.columns = [str(col) for col in train_df.columns]\n",
    "    train_df.to_feather('audio_train/padded_train_15_{}.feather'.format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
