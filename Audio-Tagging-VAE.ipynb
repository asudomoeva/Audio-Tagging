{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore', category=ImportWarning)\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import edward2 as ed\n",
    "from tensorflow.estimator.inputs import numpy_input_fn\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_SECONDS = 15\n",
    "WAV_SAMPLE_RATE = 22050\n",
    "WAV_SHAPE = [WAV_SECONDS * WAV_SAMPLE_RATE, 1]  # Time-steps X Features\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 1\n",
    "BASE_DEPTH = 12\n",
    "LATENT_DIMENSIONS = 8\n",
    "ACTIVATION = \"leaky_relu\"\n",
    "\n",
    "\n",
    "MODEL_DIR=\"/data/tensorflow/vae\"\n",
    "DATA_DIR=\"/data/tensorflow/vae/data\"\n",
    "MAX_STEPS=501\n",
    "VIZ_STEPS=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _softplus_inverse(x):\n",
    "  \"\"\"Helper which computes the function inverse of `tf.nn.softplus`.\"\"\"\n",
    "  return tf.log(tf.math.expm1(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoders aka Inference Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cnn_encoder(activation, latent_size, base_depth):\n",
    "  \"\"\"Creates the encoder function.\n",
    "  Args:\n",
    "    activation: Activation function in hidden layers.\n",
    "    latent_size: The dimensionality of the encoding.\n",
    "    base_depth: The lowest depth for a layer.\n",
    "  Returns:\n",
    "    encoder: A `callable` mapping a `Tensor` of images to a\n",
    "      `tfd.Distribution` instance over encodings.\n",
    "  \"\"\"\n",
    "  conv = functools.partial(\n",
    "      tf.keras.layers.Conv1D, padding=\"SAME\", activation=activation)\n",
    "\n",
    "  encoder_net = tf.keras.Sequential([\n",
    "      conv(base_depth, 5, 1),\n",
    "      conv(base_depth, 5, 2),\n",
    "      conv(2 * base_depth, 5, 1),\n",
    "      conv(2 * base_depth, 5, 2),\n",
    "      conv(4 * latent_size, 3, padding=\"VALID\"),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(2*latent_size, activation=None),\n",
    "  ])\n",
    "\n",
    "  def encoder(images):\n",
    "    images = tf.reshape(images, (-1, WAV_SHAPE[0], 1))\n",
    "    net = encoder_net(images)\n",
    "    \"\"\"\n",
    "    return tfd.MultivariateNormalDiag(\n",
    "        loc=net[..., :latent_size],\n",
    "        scale_diag=tf.nn.softplus(net[..., latent_size:] +\n",
    "                                  _softplus_inverse(1.0)),\n",
    "        name=\"code\")\n",
    "    \"\"\"\n",
    "    return tfd.Normal(\n",
    "        loc=net[..., :latent_size],\n",
    "        scale=tf.nn.softplus(net[..., latent_size:] + _softplus_inverse(1.0)),\n",
    "        name=\"code\")\n",
    "\n",
    "  return encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoders aka Generative Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cnn_decoder(activation, latent_size, output_shape, base_depth):\n",
    "  \"\"\"Creates the decoder function.\n",
    "  Args:\n",
    "    activation: Activation function in hidden layers.\n",
    "    latent_size: Dimensionality of the encoding.\n",
    "    output_shape: The output image shape.\n",
    "    base_depth: Smallest depth for a layer.\n",
    "  Returns:\n",
    "    decoder: A `callable` mapping a `Tensor` of encodings to a\n",
    "      `tfd.Distribution` instance over images.\n",
    "  \"\"\"\n",
    "  conv = functools.partial(\n",
    "      tf.keras.layers.Conv1D, padding=\"SAME\", activation=activation)\n",
    "    \n",
    "  decoder_net = tf.keras.Sequential([\n",
    "      conv(2 * base_depth, 7, padding=\"VALID\"),\n",
    "      tf.keras.layers.UpSampling1D(size=2),\n",
    "      conv(2 * base_depth, 5),\n",
    "      tf.keras.layers.UpSampling1D(size=2),\n",
    "      conv(2 * base_depth, 5, 2),\n",
    "      tf.keras.layers.UpSampling1D(size=2),\n",
    "      conv(base_depth, 5),\n",
    "      tf.keras.layers.UpSampling1D(size=2),\n",
    "      conv(base_depth, 5, 2),\n",
    "      tf.keras.layers.UpSampling1D(size=2),\n",
    "      conv(base_depth, 5),\n",
    "      tf.keras.layers.UpSampling1D(size=2),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(2*WAV_SHAPE[0], activation=None),\n",
    "  ])\n",
    "\n",
    "  def decoder(codes):\n",
    "    original_shape = tf.shape(codes)\n",
    "    codes = tf.reshape(codes, (-1, latent_size, 1))\n",
    "    net = decoder_net(codes)\n",
    "    \"\"\"\n",
    "    return tfd.Independent(tfd.Bernoulli(logits=logits),\n",
    "                           reinterpreted_batch_ndims=len(output_shape),\n",
    "                           name=\"image\")\n",
    "    \"\"\"\n",
    "    return tfd.Normal(\n",
    "        loc=net[..., :WAV_SHAPE[0]],\n",
    "        scale=tf.nn.softplus(net[..., WAV_SHAPE[0]:] + _softplus_inverse(1.0)),\n",
    "        name=\"wav\")\n",
    "\n",
    "  return decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Estimator model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode, params, config):\n",
    "  \"\"\"Builds the model function for use in an estimator.\n",
    "  Arguments:\n",
    "    features: The input features for the estimator.\n",
    "    labels: The labels, unused here.\n",
    "    mode: Signifies whether it is train or test or predict.\n",
    "    params: Some hyperparameters as a dictionary.\n",
    "    config: The RunConfig, unused here.\n",
    "  Returns:\n",
    "    EstimatorSpec: A tf.estimator.EstimatorSpec instance.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  encoder = make_cnn_encoder(params[\"activation\"],\n",
    "                             params[\"latent_size\"],\n",
    "                             params[\"base_depth\"])\n",
    "  decoder = make_cnn_decoder(params[\"activation\"],\n",
    "                             params[\"latent_size\"],\n",
    "                             WAV_SHAPE,\n",
    "                             params[\"base_depth\"])\n",
    "  latent_prior = tfd.MultivariateNormalDiag(\n",
    "        loc=tf.zeros([params[\"latent_size\"]]),\n",
    "        scale_identity_multiplier=1.0\n",
    "  )\n",
    "\n",
    "  approx_posterior = encoder(features)\n",
    "  approx_posterior_sample = approx_posterior.sample(1)#params[\"n_samples\"])\n",
    "  decoder_likelihood = decoder(approx_posterior_sample)\n",
    "\n",
    "  # `distortion` is just the negative log likelihood.\n",
    "  distortion = -decoder_likelihood.log_prob(features)\n",
    "  avg_distortion = tf.reduce_mean(distortion)\n",
    "  tf.summary.scalar(\"distortion\", avg_distortion)\n",
    "\n",
    "  rate = (approx_posterior.log_prob(approx_posterior_sample)\n",
    "        - latent_prior.log_prob(approx_posterior_sample))\n",
    "  avg_rate = tf.reduce_mean(rate)\n",
    "  tf.summary.scalar(\"rate\", avg_rate)\n",
    "\n",
    "  #elbo_local = -(rate + distortion)\n",
    "\n",
    "  #elbo = tf.reduce_mean(elbo_local)\n",
    "  elbo = -(avg_rate + avg_distortion)\n",
    "  loss = -elbo\n",
    "  tf.summary.scalar(\"elbo\", elbo)\n",
    "\n",
    "  \"\"\"\n",
    "  importance_weighted_elbo = tf.reduce_mean(\n",
    "      tf.reduce_logsumexp(elbo_local, axis=0) -\n",
    "      tf.log(tf.to_float(params[\"n_samples\"])))\n",
    "  tf.summary.scalar(\"elbo/importance_weighted\", importance_weighted_elbo)\n",
    "  \"\"\"\n",
    "    \n",
    "  random_wav = decoder(latent_prior.sample(16))\n",
    "  tf.summary.audio(\"random/sample\", random_wav.sample(), sample_rate=22050)\n",
    "  tf.summary.audio(\"random/mean\", random_wav.mean(), sample_rate=22050)\n",
    "\n",
    "  # Perform variational inference by minimizing the -ELBO.\n",
    "  global_step = tf.train.get_or_create_global_step()\n",
    "  learning_rate = tf.train.cosine_decay(params[\"learning_rate\"], global_step,\n",
    "                                        params[\"max_steps\"])\n",
    "  tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "  optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "  train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    predictions = {\n",
    "        'encoded_sample': approx_posterior.sample(1), \n",
    "        'encoded_mean': approx_posterior.mean(), \n",
    "        'reconstructed_sample': decoder_likelihood.sample(1), \n",
    "        'reconstructed_mean': decoder_likelihood.mean(),\n",
    "    }\n",
    "  else:\n",
    "    predictions = None\n",
    "\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      loss=loss,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={\n",
    "          \"elbo\": tf.metrics.mean(elbo),\n",
    "          #\"elbo/importance_weighted\": tf.metrics.mean(importance_weighted_elbo),\n",
    "          \"rate\": tf.metrics.mean(avg_rate),\n",
    "          \"distortion\": tf.metrics.mean(avg_distortion),\n",
    "      },\n",
    "      predictions=predictions,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data(file_name):\n",
    "    import feather  # Super fast way to read/write tabular data\n",
    "    training_data = feather.read_dataframe(file_name).set_index('index')\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>330742</th>\n",
       "      <th>330743</th>\n",
       "      <th>330744</th>\n",
       "      <th>330745</th>\n",
       "      <th>330746</th>\n",
       "      <th>330747</th>\n",
       "      <th>330748</th>\n",
       "      <th>330749</th>\n",
       "      <th>label</th>\n",
       "      <th>manually_verified</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30114da8.wav</th>\n",
       "      <td>-0.002574</td>\n",
       "      <td>0.006757</td>\n",
       "      <td>-0.006870</td>\n",
       "      <td>0.009043</td>\n",
       "      <td>-0.008641</td>\n",
       "      <td>0.013116</td>\n",
       "      <td>0.004403</td>\n",
       "      <td>-0.032082</td>\n",
       "      <td>0.020017</td>\n",
       "      <td>-0.009521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Hi-hat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288d0dff.wav</th>\n",
       "      <td>0.002158</td>\n",
       "      <td>-0.000712</td>\n",
       "      <td>0.004873</td>\n",
       "      <td>-0.001874</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>0.003193</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.006986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Hi-hat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0ef9a602.wav</th>\n",
       "      <td>-0.074835</td>\n",
       "      <td>0.162536</td>\n",
       "      <td>-0.357083</td>\n",
       "      <td>0.246719</td>\n",
       "      <td>-0.096885</td>\n",
       "      <td>-0.116554</td>\n",
       "      <td>0.189366</td>\n",
       "      <td>-0.277244</td>\n",
       "      <td>0.122169</td>\n",
       "      <td>0.095103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Hi-hat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f6a82a6c.wav</th>\n",
       "      <td>-0.002069</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.005921</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.001272</td>\n",
       "      <td>0.006814</td>\n",
       "      <td>0.011007</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007527</td>\n",
       "      <td>-0.006673</td>\n",
       "      <td>-0.008705</td>\n",
       "      <td>-0.012213</td>\n",
       "      <td>-0.013543</td>\n",
       "      <td>-0.010741</td>\n",
       "      <td>-0.006116</td>\n",
       "      <td>-0.002419</td>\n",
       "      <td>Hi-hat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f01d4739.wav</th>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>-0.000261</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Hi-hat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 330752 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0         1         2         3         4         5  \\\n",
       "index                                                                      \n",
       "30114da8.wav -0.002574  0.006757 -0.006870  0.009043 -0.008641  0.013116   \n",
       "288d0dff.wav  0.002158 -0.000712  0.004873 -0.001874  0.005291  0.001182   \n",
       "0ef9a602.wav -0.074835  0.162536 -0.357083  0.246719 -0.096885 -0.116554   \n",
       "f6a82a6c.wav -0.002069  0.003097  0.000513  0.005921  0.002216  0.001272   \n",
       "f01d4739.wav -0.000034 -0.000019 -0.000018 -0.000084  0.000016 -0.000038   \n",
       "\n",
       "                     6         7         8         9        ...          \\\n",
       "index                                                       ...           \n",
       "30114da8.wav  0.004403 -0.032082  0.020017 -0.009521        ...           \n",
       "288d0dff.wav  0.001999  0.003193  0.001588  0.006986        ...           \n",
       "0ef9a602.wav  0.189366 -0.277244  0.122169  0.095103        ...           \n",
       "f6a82a6c.wav  0.006814  0.011007  0.003781  0.004145        ...           \n",
       "f01d4739.wav  0.000111  0.000051 -0.000261 -0.000125        ...           \n",
       "\n",
       "                330742    330743    330744    330745    330746    330747  \\\n",
       "index                                                                      \n",
       "30114da8.wav  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "288d0dff.wav  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "0ef9a602.wav  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "f6a82a6c.wav -0.007527 -0.006673 -0.008705 -0.012213 -0.013543 -0.010741   \n",
       "f01d4739.wav  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "                330748    330749   label  manually_verified  \n",
       "index                                                        \n",
       "30114da8.wav  0.000000  0.000000  Hi-hat                  1  \n",
       "288d0dff.wav  0.000000  0.000000  Hi-hat                  1  \n",
       "0ef9a602.wav  0.000000  0.000000  Hi-hat                  1  \n",
       "f6a82a6c.wav -0.006116 -0.002419  Hi-hat                  0  \n",
       "f01d4739.wav  0.000000  0.000000  Hi-hat                  0  \n",
       "\n",
       "[5 rows x 330752 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_preprocessed_data('padded_train_15_sample.feather')\n",
    "y_train = data['label']\n",
    "x_train = data.drop(['label', 'manually_verified'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = numpy_input_fn(\n",
    "    x_train.values.astype(np.float32), \n",
    "    shuffle=True, \n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(input_fn):\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'latent_size': LATENT_DIMENSIONS,\n",
    "        'activation': ACTIVATION,\n",
    "        'base_depth': BASE_DEPTH,\n",
    "        'max_steps': MAX_STEPS,\n",
    "    }\n",
    "    params[\"activation\"] = getattr(tf.nn, params[\"activation\"])\n",
    "    \"\"\"\n",
    "    if FLAGS.delete_existing and tf.gfile.Exists(MODEL_DIR):\n",
    "        tf.logging.warn(\"Deleting old log directory at {}\".format(MODEL_DIR))\n",
    "        tf.gfile.DeleteRecursively(MODEL_DIR)\n",
    "        tf.gfile.MakeDirs(MODEL_DIR)\n",
    "    \"\"\"\n",
    "    \n",
    "    estimator = tf.estimator.Estimator(\n",
    "      cnn_model_fn,\n",
    "      params=params,\n",
    "      config=tf.estimator.RunConfig(\n",
    "          model_dir=MODEL_DIR,\n",
    "          save_checkpoints_steps=VIZ_STEPS,\n",
    "      ),\n",
    "    )\n",
    "    for _ in range(MAX_STEPS // VIZ_STEPS):\n",
    "        estimator.train(input_fn=input_fn, steps=VIZ_STEPS)\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/data/tensorflow/vae', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 50, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f58967b54a8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From /home/smcclain1/venv/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/smcclain1/venv/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From /home/smcclain1/venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /data/tensorflow/vae/model.ckpt.\n",
      "INFO:tensorflow:loss = 10.051489, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 50 into /data/tensorflow/vae/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 8.421153.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/vae/model.ckpt-50\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 50 into /data/tensorflow/vae/model.ckpt.\n",
      "INFO:tensorflow:loss = 8.592899, step = 51\n",
      "INFO:tensorflow:Saving checkpoints for 100 into /data/tensorflow/vae/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 8.194532.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/vae/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 100 into /data/tensorflow/vae/model.ckpt.\n",
      "INFO:tensorflow:loss = 8.12851, step = 101\n",
      "INFO:tensorflow:Saving checkpoints for 150 into /data/tensorflow/vae/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 6.37552.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/vae/model.ckpt-150\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 150 into /data/tensorflow/vae/model.ckpt.\n",
      "INFO:tensorflow:loss = 6.253061, step = 151\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /data/tensorflow/vae/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 26.97768.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/vae/model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /data/tensorflow/vae/model.ckpt.\n",
      "INFO:tensorflow:loss = 7.8685064, step = 201\n",
      "INFO:tensorflow:Saving checkpoints for 250 into /data/tensorflow/vae/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 9.571322.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/vae/model.ckpt-250\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 250 into /data/tensorflow/vae/model.ckpt.\n",
      "INFO:tensorflow:loss = 28.01814, step = 251\n",
      "INFO:tensorflow:Saving checkpoints for 300 into /data/tensorflow/vae/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 25.16776.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/vae/model.ckpt-300\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 300 into /data/tensorflow/vae/model.ckpt.\n",
      "INFO:tensorflow:loss = 12.855383, step = 301\n",
      "INFO:tensorflow:Saving checkpoints for 350 into /data/tensorflow/vae/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 8.292048.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/vae/model.ckpt-350\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 350 into /data/tensorflow/vae/model.ckpt.\n",
      "INFO:tensorflow:loss = 8.77861, step = 351\n",
      "INFO:tensorflow:Saving checkpoints for 400 into /data/tensorflow/vae/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 8.288644.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/vae/model.ckpt-400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 400 into /data/tensorflow/vae/model.ckpt.\n",
      "INFO:tensorflow:loss = 8.429716, step = 401\n",
      "INFO:tensorflow:Saving checkpoints for 450 into /data/tensorflow/vae/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 18.316246.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/vae/model.ckpt-450\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 450 into /data/tensorflow/vae/model.ckpt.\n",
      "INFO:tensorflow:loss = 7.598678, step = 451\n",
      "INFO:tensorflow:Saving checkpoints for 500 into /data/tensorflow/vae/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 9.638972.\n"
     ]
    }
   ],
   "source": [
    "estimator = train_cnn(train_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Criticism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_fn = numpy_input_fn(\n",
    "    x_train.values.astype(np.float32), \n",
    "    shuffle=False, \n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /data/tensorflow/vae/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "predictions = list(\n",
    "    estimator.predict(input_fn=predict_input_fn, yield_single_examples=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_encodings = pd.DataFrame([predictions[i]['encoded_sample'][0][0] for i in range(len(predictions))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = y_train.to_frame()\n",
    "y_df['label_cat'] = y_df['label'].astype('category')\n",
    "y_df['label_int'] = y_df['label_cat'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08658536585365853"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(predicted_encodings.values, y_df['label_int'].values)\n",
    "lr.score(predicted_encodings.values, y_df['label_int'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the latent representation is not linearly separable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998780487804878"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = RandomForestClassifier()\n",
    "r.fit(predicted_encodings.values, y_df['label_int'].values)\n",
    "r.score(predicted_encodings.values, y_df['label_int'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's more like it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO repeat the tests on *OUT OF SAMPLE* data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_padding(descriptive_df, mode='train'):\n",
    "    # '' using 15 second as our standard for padded audio files\n",
    "    # '' this function only returns a list of np.arrays without any label association (for faster run time)\n",
    "    names = []\n",
    "    audio = []\n",
    "    seconds = 15\n",
    "    sample_rate = 22050\n",
    "    max_len = seconds * sample_rate\n",
    "    for row in descriptive_df.itertuples():\n",
    "        file_path = 'audio_{}/audio_{}/{}'.format(mode, mode, row.Index)\n",
    "        data = librosa.load(file_path)[0][:max_len]\n",
    "        duration = data.shape[0]\n",
    "        padding_len = max_len - duration\n",
    "        padding = np.zeros(padding_len)\n",
    "        data = np.append(data, padding)\n",
    "        audio.append(data)\n",
    "        names.append(row.Index)\n",
    "    return pd.DataFrame(data=audio, index=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(label, count):\n",
    "    return audio_padding(descriptive_df.loc[descriptive_df.label == label].sample(n=count, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive_df = pd.read_csv('train_descriptive.csv', index_col=0).set_index('fname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random 20 WAV files for each label\n",
    "train_df = pd.concat([get_df(l, 20) for l in descriptive_df['label'].unique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.join(descriptive_df[['label', 'manually_verified']])\n",
    "train_df = train_df.reset_index()\n",
    "train_df.columns = [str(col) for col in train_df.columns]\n",
    "train_df.to_feather('padded_train_15_sample.feather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
